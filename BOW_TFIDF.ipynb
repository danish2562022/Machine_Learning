{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsBnMgoKdRHP"
      },
      "source": [
        "### Implementing Bag of Words without using sklearn library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPag30Y0dYyC"
      },
      "source": [
        "## Q. What is Bag of Word?\n",
        "\n",
        "It is a technique of conversion of text into numerical features. \n",
        "\n",
        "It describes the occurence of words within a document. \n",
        "\n",
        "It tells two things:\n",
        "\n",
        "    1. Whether a word is present or not\n",
        "    2. Frequency of word\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zgppHt-dXv9"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_HrH_x2eNG8"
      },
      "source": [
        "# fit method of sklearn\n",
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "def fit(dataset):\n",
        "    unique_words=set()\n",
        "    if isinstance(dataset,(list,)):\n",
        "        for row in dataset:\n",
        "            for word in row.split(\" \"):\n",
        "                if len(word)<2:\n",
        "                    continue\n",
        "                unique_words.add(word)\n",
        "        unique_words=sorted(list(unique_words))\n",
        "        vocab={j:i for i,j in enumerate(unique_words)}\n",
        "\n",
        "        return vocab\n",
        "    else:\n",
        "        print(\"enter list of sentence\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qcZUBdXfUrL"
      },
      "source": [
        "# Transform method in BOW\n",
        "def transform(dataset,vocab):\n",
        "    rows = []\n",
        "    columns = []\n",
        "    values = []\n",
        "    if isinstance(dataset,(list,)):\n",
        "        for idx,row in enumerate(tqdm(dataset)):\n",
        "            word_freq=dict(Counter(row.split()))\n",
        "            for word,freq in word_freq.items():\n",
        "                if len(word)<2:\n",
        "                    continue\n",
        "                col_index=vocab.get(word,-1)\n",
        "                if col_index !=1:\n",
        "                    rows.append(idx)\n",
        "                    columns.append(col_index)\n",
        "                    values.append(freq)\n",
        "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
        "\n",
        "    else:\n",
        "        print(\"Enter list of sentence\")\n",
        "        \n",
        "                \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4GM_kidIo-t"
      },
      "source": [
        "strings = [\"the method of lagrange multipliers is the economists workhorse for solving optimization problems\",\n",
        "           \"the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\"]\n",
        "vocab = fit(strings)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLetjJrbKdTZ",
        "outputId": "910ead70-e603-4925-c295-7e07b17621cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "trns=transform(strings,vocab)\n",
        "print(list(vocab.keys()))\n",
        "print(trns.toarray())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 508.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['but', 'centerpiece', 'economic', 'economists', 'for', 'is', 'its', 'lagrange', 'method', 'multipliers', 'of', 'optimization', 'poorly', 'problems', 'solving', 'taught', 'technique', 'the', 'theory', 'unfortunately', 'usually', 'workhorse']\n",
            "[[0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 2 0 0 0 1]\n",
            " [1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DztiBC1nXyde",
        "outputId": "d877f444-f025-40c4-f49d-b40ac31d8757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "dictt={}\n",
        "for row in list(vocab.keys()):\n",
        "    dictt[row]=0\n",
        "\n",
        "\n",
        "dictt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'but': 0,\n",
              " 'centerpiece': 0,\n",
              " 'economic': 0,\n",
              " 'economists': 0,\n",
              " 'for': 0,\n",
              " 'is': 0,\n",
              " 'its': 0,\n",
              " 'lagrange': 0,\n",
              " 'method': 0,\n",
              " 'multipliers': 0,\n",
              " 'of': 0,\n",
              " 'optimization': 0,\n",
              " 'poorly': 0,\n",
              " 'problems': 0,\n",
              " 'solving': 0,\n",
              " 'taught': 0,\n",
              " 'technique': 0,\n",
              " 'the': 0,\n",
              " 'theory': 0,\n",
              " 'unfortunately': 0,\n",
              " 'usually': 0,\n",
              " 'workhorse': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kRglWbpLGyi"
      },
      "source": [
        "### Implementing TFIDF without using sklearn library\n",
        "\n",
        "\n",
        "<strong>What is TF-IDF? What is the mathematics behind TF-IDF?</strong>\n",
        "\n",
        "\n",
        "<strong>Term Frequency(TF)</strong>: It measures , how frequently a word occur in the document. If the word is appear more number of times in a document, then it give more weightage.\n",
        "\n",
        "$TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}.$\n",
        "</li>\n",
        "<li>\n",
        "<strong>Inverse Document Frequency(IDF)</strong>= It measures how important a term/word appear in the corpus. If thr word appears in many number of documents, then it holds less importance.\n",
        "For example: Word like \"is\",\"the\" ,\"and\" etc are present in almost all the documents in the corpus, so it holds less importance than the word which appear less number of times in document.\n",
        "\n",
        "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}}.$\n",
        "for numerical stabiltiy we will be changing this formula little bit\n",
        "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}+1}.$\n",
        "</li>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0YBeqSlKwmX"
      },
      "source": [
        "import math\n",
        "def transform_tfidf(dataset,vocab):\n",
        "    rows = []\n",
        "    columns = []\n",
        "    values = []\n",
        "    Number_of_documents_with_term_t={}\n",
        "    total_number_of_documents=len(dataset)\n",
        "    \n",
        "    if isinstance(dataset,(list,)):\n",
        "        \n",
        "        \n",
        "        #IDF calculation\n",
        "        for key in list(vocab.keys()):\n",
        "            Number_of_documents_with_term_t[key]=0\n",
        "           \n",
        "        for idx,row in enumerate(tqdm(dataset)):\n",
        "            #TF calculation\n",
        "            word_freq=dict(Counter(row.split()))\n",
        "            for word, freq in word_freq.items():\n",
        "                 \n",
        "                    Number_of_documents_with_term_t[word]= Number_of_documents_with_term_t[word]+1\n",
        "           \n",
        "              \n",
        "\n",
        "\n",
        "        #TF-IDF calculation\n",
        "        for idx,row in enumerate(tqdm(dataset)):\n",
        "            #TF calculation\n",
        "            word_freq=dict(Counter(row.split()))\n",
        "            \n",
        "           \n",
        "            for word, freq in word_freq.items():\n",
        "               \n",
        "                col_index=vocab.get(word,-1)\n",
        "\n",
        "                if col_index !=-1:\n",
        "                    rows.append(idx)\n",
        "                    columns.append(col_index)\n",
        "                    TF=freq/len(row)\n",
        "                    \n",
        "                    IDF=math.log((1+total_number_of_documents)/(1+Number_of_documents_with_term_t[word]))+1\n",
        "                    values.append(TF*IDF)\n",
        "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
        "    else:\n",
        "        print(\"Enter list of document\")           \n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H1kfkIuhOWn",
        "outputId": "9cf6e48c-4507-4f89-d4b6-e971a089e605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus = [\n",
        "'this is the first document',\n",
        "'this document is the second document',\n",
        "'and this is the third one',\n",
        "'is this the first document',\n",
        "]\n",
        "vocab=fit(corpus)\n",
        "print(vocab)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sbRU80Zj_-H"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QOWcDqfkUGu"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPrXVKoevKrv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}